# 📚 Latent Reasoning Papers

A curated collection of papers related to **latent reasoning** in Large Language Models (LLMs). This repository provides a structured overview of relevant publications.

## 📌 Introduction

Latent reasoning in LLMs refers to the implicit, hidden, or unobservable reasoning processes that occur within these models when generating responses, making predictions, or solving problems. Unlike explicit reasoning (e.g., Chain-of-Thought prompting), latent reasoning operates beneath the surface, embedded in the model’s internal representations.

---

## 📂 **Paper Collection**

### **🔹 2025 Papers**


- **[Reasoning with Latent Thoughts: On the Power of Looped Transformers](https://arxiv.org/abs/2502.17416)**  
  - Explores how looping mechanisms in transformers improve latent reasoning by refining internal representations over multiple iterations.

- **[Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs](https://arxiv.org/pdf/2502.21030)**  
  - Introduces a memory-augmented model that stores latent reasoning traces for complex multi-step inference.

- **[Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171)**  
  - Investigates adaptive recurrent depth in transformers to enhance latent reasoning capabilities.

- **[LLM Pretraining with Continuous Concepts](https://arxiv.org/pdf/2502.08524)**  
  - Proposes an alternative pretraining paradigm that embeds conceptual understanding in a continuous latent space.

- **[Efficient Reasoning with Hidden Thinking](https://arxiv.org/pdf/2501.19201)**  
  - Studies how hidden states in LLMs encode intermediate reasoning steps, enabling more efficient inference.

- **[SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs](https://arxiv.org/pdf/2502.12134)**  
  - Proposes a “soft” version of Chain-of-Thought reasoning using latent state interpolation instead of discrete token steps.

---

### **🔹 2024 Papers**


- **[Byte Latent Transformer: Patches Scale Better than Tokens](https://arxiv.org/pdf/2412.09871)**  
  - Argues that processing information at the byte level instead of token granularity leads to improved latent representation learning.

- **[Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)**  
  - Introduces a method where LLMs reason in a continuous embedding space instead of discrete token sequences.

- **[Large Concept Models: Language Modeling in a Sentence Representation Space](https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/)**  
  - Discusses how sentence-level representations enable more abstract, conceptual reasoning.

---

## 🕮 **Embedding-Enhanced Reasoning**

### **🔹 2025 Papers**
- **[Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning](https://arxiv.org/pdf/2502.03275)**  
  - Explores a hybrid architecture that combines explicit tokens and latent thought vectors for reasoning.

- **[Scalable Language Models with Posterior Inference of Latent Thought Vectors](https://arxiv.org/pdf/2502.01567)**  
  - Introduces a posterior inference framework to optimize latent thought representations for LLMs.

### **🔹 2024 Papers**
- **[Latent Space Chain-of-Embedding Enables Output-Free LLM Self-Evaluation](https://arxiv.org/pdf/2410.13640v1)**  
  - Proposes a method where LLMs self-evaluate their own reasoning by analyzing latent embeddings instead of generating outputs.

- **[Think Before You Speak: Training Language Models with Pause Tokens](https://arxiv.org/pdf/2310.02226)**  
  - Investigates the effect of inserting “pause” tokens that encourage deeper latent reasoning before producing output.

- **[Guiding Language Model Reasoning with Planning Tokens](https://arxiv.org/pdf/2310.05707)**  
  - Explores how explicit planning tokens influence latent reasoning processes.

---

## 📑 **Additional Resources**
- 📌 **Surveys & Tutorials**: Coming soon!
- 🛠️ **Datasets & Benchmarks**: Coming soon!
- 🔗 **Related GitHub Repositories**: [Awesome-LLM-Reasoning](https://github.com/atfortes/Awesome-LLM-Reasoning)

---
