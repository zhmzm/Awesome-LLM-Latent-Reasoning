# ðŸ“š Latent Reasoning Papers

A curated collection of papers related to **latent reasoning** in LLMs. This repository aims to provide an list of related publications.

## ðŸ“Œ Introduction

Latent reasoning in Large Language Models (LLMs) refers to the implicit, hidden, or unobservable reasoning processes that occur within these models when generating responses, making predictions, or solving problems. Unlike explicit reasoning, where the model follows a clear chain of thought, latent reasoning operates beneath the surface, embedded in the modelâ€™s internal representations.

## ðŸ•® Latent Reasoning Papers

### 2025 

[REASONING WITH LATENT THOUGHTS: ON THE  POWER OF LOOPED TRANSFORMERS](https://arxiv.org/abs/2502.17416)

[Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs](https://arxiv.org/pdf/2502.21030)

[Scaling up Test-Time Compute with Latent Reasoning:  A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171)

[LLM Pretraining with Continuous Concepts](https://arxiv.org/pdf/2502.08524)

[Efficient Reasoning with Hidden Thinking](https://arxiv.org/pdf/2501.19201)

[SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs](https://arxiv.org/pdf/2502.12134)

### 2024 

[Byte latent transformer: Patches scale better than tokens](https://arxiv.org/pdf/2412.09871)

[Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)

[Large Concept Models: Language Modeling in a Sentence Representation Space](https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/)

## ðŸ•® Embedding Enhanced Reasoning

### 2025 

[Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning](https://arxiv.org/pdf/2502.03275)

[Scalable Language Models with Posterior Inference of Latent Thought Vectors](https://arxiv.org/pdf/2502.01567)

### 2024 

[LATENT SPACE CHAIN-OF-EMBEDDING ENABLES OUTPUT-FREE LLM SELF-EVALUATION](https://arxiv.org/pdf/2410.13640v1)

[Think before you speak: Training language models with pause tokens](https://arxiv.org/pdf/2310.02226)

[Guiding Language Model Reasoning with Planning Tokens](https://arxiv.org/pdf/2310.05707)



