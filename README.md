# ðŸ“š Latent Reasoning Papers

A curated collection of papers related to **latent reasoning** in LLMs. This repository aims to provide an list of related publications.

## ðŸ“Œ Introduction

Latent reasoning in Large Language Models (LLMs) refers to the implicit, hidden, or unobservable reasoning processes that occur within these models when generating responses, making predictions, or solving problems. Unlike explicit reasoning, where the model follows a clear chain of thought, latent reasoning operates beneath the surface, embedded in the modelâ€™s internal representations.

## ðŸ•® Latent Reasoning Papers

### 2025 

[Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning](https://arxiv.org/pdf/2502.03275)

[LLM Pretraining with Continuous Concepts](https://arxiv.org/pdf/2502.08524)

[Efficient Reasoning with Hidden Thinking](https://arxiv.org/pdf/2501.19201)

[SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs](https://arxiv.org/pdf/2502.12134)

### 2024 

[Byte latent transformer: Patches scale better than tokens](https://arxiv.org/pdf/2412.09871)

[Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)

[Large Concept Models: Language Modeling in a Sentence Representation Space](https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/)

## ðŸ•® Embedding Enhanced Reasoning

### 2025 

[Scalable Language Models with Posterior Inference of Latent Thought Vectors](https://arxiv.org/pdf/2502.01567)

### 2024 
[Think before you speak: Training language models with pause tokens](https://arxiv.org/pdf/2310.02226)

[Guiding Language Model Reasoning with Planning Tokens](https://arxiv.org/pdf/2310.05707)


